import pandas as pd
import numpy as np
import os
import joblib
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# --- Cáº¤U HÃŒNH ---
FILE_DATA = r"D:\PHAN_TICH_DU_LIEU\data\bitcoin_processed.csv"
MODEL_PATH = "models/bitcoin_lstm.h5"
SCALER_PATH = "models/scaler.pkl"

# Tham sá»‘ mÃ´ hÃ¬nh
LOOKBACK = 60       # NhÃ¬n láº¡i 60 cÃ¢y náº¿n (5 tiáº¿ng)
BATCH_SIZE = 64     # Há»c 64 dÃ²ng cÃ¹ng lÃºc (TÄƒng tá»‘c Ä‘á»™)
EPOCHS = 100        # Cho phÃ©p há»c tá»‘i Ä‘a 100 láº§n (Sáº½ tá»± dá»«ng náº¿u Ä‘Ã£ giá»i)
TEST_SIZE = 0.2     # 20% dá»¯ liá»‡u Ä‘á»ƒ thi

def create_sequences(data, lookback):
    """HÃ m cáº¯t dá»¯ liá»‡u thÃ nh cÃ¡c Ä‘oáº¡n 60 náº¿n"""
    X, y = [], []
    # Dá»¯ liá»‡u vÃ o: Táº¥t cáº£ cÃ¡c cá»™t TRá»ª cá»™t cuá»‘i (target)
    # Dá»¯ liá»‡u ra: Chá»‰ cá»™t cuá»‘i (target)
    for i in range(len(data) - lookback):
        X.append(data[i:i+lookback, :-1]) 
        y.append(data[i+lookback, -1])    
    return np.array(X), np.array(y)

def train_best_model():
    print("ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N (CHáº¾ Äá»˜ CHUYÃŠN NGHIá»†P)...")

    # 1. Äá»c dá»¯ liá»‡u
    if not os.path.exists(FILE_DATA):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {FILE_DATA}")
        return

    df = pd.read_csv(FILE_DATA)
    
    # Bá» cá»™t thá»i gian, chá»‰ giá»¯ láº¡i sá»‘ liá»‡u
    if 'timestamp' in df.columns:
        df = df.drop(columns=['timestamp'])
    
    # 2. CHIA Táº¬P TRAIN/TEST TRÆ¯á»šC (QUAN TRá»ŒNG)
    # Äá»ƒ trÃ¡nh viá»‡c Scaler "nhÃ¬n trá»™m" dá»¯ liá»‡u tÆ°Æ¡ng lai
    train_size = int(len(df) * (1 - TEST_SIZE))
    train_df = df.iloc[:train_size]
    test_df = df.iloc[train_size:]
    
    print(f"ğŸ“Š Dá»¯ liá»‡u Train: {len(train_df)} dÃ²ng | Test: {len(test_df)} dÃ²ng")

    # 3. Chuáº©n hÃ³a dá»¯ liá»‡u (Scaling)
    # Chá»‰ há»c tá»« táº­p Train, sau Ä‘Ã³ Ã¡p dá»¥ng kiáº¿n thá»©c Ä‘Ã³ lÃªn táº­p Test
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaler.fit(train_df) 
    
    scaled_train = scaler.transform(train_df)
    scaled_test = scaler.transform(test_df)
    
    # LÆ°u Scaler láº¡i Ä‘á»ƒ dÃ¹ng cho Bot thá»±c táº¿
    os.makedirs("models", exist_ok=True)
    joblib.dump(scaler, SCALER_PATH)
    print("ğŸ’¾ ÄÃ£ lÆ°u bá»™ chuáº©n hÃ³a Scaler.")

    # 4. Táº¡o chuá»—i dá»¯ liá»‡u (Sliding Window)
    print("âš™ï¸ Äang cáº¯t dá»¯ liá»‡u thÃ nh chuá»—i...")
    X_train, y_train = create_sequences(scaled_train, LOOKBACK)
    X_test, y_test = create_sequences(scaled_test, LOOKBACK)
    
    # Kiá»ƒm tra kÃ­ch thÆ°á»›c láº§n cuá»‘i
    print(f"   -> Input Train: {X_train.shape} (Máº«u, Thá»i gian, Äáº·c trÆ°ng)")
    print(f"   -> Output Train: {y_train.shape}")

    # 5. XÃ¢y dá»±ng bá»™ nÃ£o LSTM (Architecture)
    model = Sequential()
    
    # Lá»›p 1: LSTM máº¡nh máº½
    model.add(LSTM(units=64, return_sequences=True, input_shape=(LOOKBACK, X_train.shape[2])))
    model.add(Dropout(0.3)) # QuÃªn 30% Ä‘á»ƒ trÃ¡nh há»c váº¹t
    
    # Lá»›p 2: LSTM tinh chá»‰nh
    model.add(LSTM(units=64, return_sequences=False))
    model.add(Dropout(0.3))
    
    # Lá»›p 3: Äáº§u ra
    model.add(Dense(25)) # Lá»›p trung gian
    model.add(Dense(1))  # GiÃ¡ dá»± bÃ¡o
    
    # DÃ¹ng Adam vá»›i learning rate nhá» Ä‘á»ƒ há»c ká»¹
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

    # 6. CÃ¡c cÆ¡ cháº¿ tá»± Ä‘á»™ng (Callbacks) - VÅ¨ KHÃ BÃ Máº¬T
    callbacks = [
        # LÆ°u láº¡i model táº¡i thá»i Ä‘iá»ƒm tá»‘t nháº¥t (val_loss tháº¥p nháº¥t)
        ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor='val_loss', mode='min', verbose=1),
        
        # Náº¿u sau 10 láº§n há»c mÃ  khÃ´ng tiáº¿n bá»™ -> Dá»«ng láº¡i
        EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),
        
        # Náº¿u sau 5 láº§n há»c mÃ  khÃ´ng tiáº¿n bá»™ -> Giáº£m tá»‘c Ä‘á»™ há»c xuá»‘ng 2 láº§n
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ]

    # 7. Báº¯t Ä‘áº§u Train
    print("ğŸ”¥ Äang train... (AI sáº½ tá»± Ä‘á»™ng dá»«ng khi há»c xong)")
    history = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )

    print("-" * 50)
    print(f"âœ… HUáº¤N LUYá»†N XONG! Model xá»‹n nháº¥t Ä‘Ã£ lÆ°u táº¡i: {MODEL_PATH}")

if __name__ == "__main__":
    train_best_model()